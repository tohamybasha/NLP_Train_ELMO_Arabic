{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train ELMO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_AC6OKKD2jC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDu3q5FAqW3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 1k8G5bOTWHTlV7UWAUHLZjpGP-yy4iyeh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERTha8J3UPR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://drive.google.com/file/d/1qPnDc3SJ473bqfATE4AfkdkWOQkud6e1/view?usp=sharing\n",
        "https://drive.google.com/file/d/1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H/view?usp=sharing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3JxOwPqsu-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1k8G5bOTWHTlV7UWAUHLZjpGP-yy4iyeh' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1k8G5bOTWHTlV7UWAUHLZjpGP-yy4iyeh\" -O unlabeled_10M_1.tsv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_huEYcUUBmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qPnDc3SJ473bqfATE4AfkdkWOQkud6e1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1qPnDc3SJ473bqfATE4AfkdkWOQkud6e1\" -O dev_labeled.tsv && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQE5y3lzU_8e",
        "colab_type": "code",
        "outputId": "a0fde7f1-b114-430b-905d-b304fc2b203c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H\" -O train_labeled.tsv && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-23 00:00:05--  https://docs.google.com/uc?export=download&confirm=&id=1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.203.101, 172.217.203.113, 172.217.203.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.203.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-ag-docs.googleusercontent.com/docs/securesc/nbi086lor53vr0ukqaiq5ld3lni5ruic/0t094896m7t3vejj08felvpsn9vn01fl/1587600000000/12684776072375856457/11715250907945946374Z/1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H?e=download [following]\n",
            "--2020-04-23 00:00:05--  https://doc-0o-ag-docs.googleusercontent.com/docs/securesc/nbi086lor53vr0ukqaiq5ld3lni5ruic/0t094896m7t3vejj08felvpsn9vn01fl/1587600000000/12684776072375856457/11715250907945946374Z/1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H?e=download\n",
            "Resolving doc-0o-ag-docs.googleusercontent.com (doc-0o-ag-docs.googleusercontent.com)... 173.194.216.132, 2607:f8b0:400c:c12::84\n",
            "Connecting to doc-0o-ag-docs.googleusercontent.com (doc-0o-ag-docs.googleusercontent.com)|173.194.216.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=504h2pdalqiso&continue=https://doc-0o-ag-docs.googleusercontent.com/docs/securesc/nbi086lor53vr0ukqaiq5ld3lni5ruic/0t094896m7t3vejj08felvpsn9vn01fl/1587600000000/12684776072375856457/11715250907945946374Z/1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H?e%3Ddownload&hash=c83aln07eos5pv3ab5mc8gnn0qge0644 [following]\n",
            "--2020-04-23 00:00:05--  https://docs.google.com/nonceSigner?nonce=504h2pdalqiso&continue=https://doc-0o-ag-docs.googleusercontent.com/docs/securesc/nbi086lor53vr0ukqaiq5ld3lni5ruic/0t094896m7t3vejj08felvpsn9vn01fl/1587600000000/12684776072375856457/11715250907945946374Z/1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H?e%3Ddownload&hash=c83aln07eos5pv3ab5mc8gnn0qge0644\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.203.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0o-ag-docs.googleusercontent.com/docs/securesc/nbi086lor53vr0ukqaiq5ld3lni5ruic/0t094896m7t3vejj08felvpsn9vn01fl/1587600000000/12684776072375856457/11715250907945946374Z/1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H?e=download&nonce=504h2pdalqiso&user=11715250907945946374Z&hash=4v63007n85t03osatkdhpcj9h58vanki [following]\n",
            "--2020-04-23 00:00:05--  https://doc-0o-ag-docs.googleusercontent.com/docs/securesc/nbi086lor53vr0ukqaiq5ld3lni5ruic/0t094896m7t3vejj08felvpsn9vn01fl/1587600000000/12684776072375856457/11715250907945946374Z/1BkJtKyrehYTeW_Meul-jhTglhNpo2A9H?e=download&nonce=504h2pdalqiso&user=11715250907945946374Z&hash=4v63007n85t03osatkdhpcj9h58vanki\n",
            "Connecting to doc-0o-ag-docs.googleusercontent.com (doc-0o-ag-docs.googleusercontent.com)|173.194.216.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/tab-separated-values]\n",
            "Saving to: â€˜train_labeled.tsvâ€™\n",
            "\n",
            "train_labeled.tsv       [ <=>                ]   3.41M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-04-23 00:00:06 (109 MB/s) - â€˜train_labeled.tsvâ€™ saved [3571892]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjEBzhGr4wiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP3It0f5GHIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpms6Rez5qoB",
        "colab_type": "text"
      },
      "source": [
        "# LOAD NADI DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkRAQKt2quDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train0 = pd.read_csv(\"/content/drive/My Drive/unlabeled tweets/unlabeled_10M_1.tsv\",delimiter='\\t',encoding='utf-8')\n",
        "for i in range(2,9):\n",
        "  data_train0 = data_train0.append(pd.read_csv(\"/content/drive/My Drive/unlabeled tweets/unlabeled_10M_\"+str(i)+\".tsv\",delimiter='\\t',encoding='utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72WnevHJrbk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train0.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz9U_KllvbjR",
        "colab_type": "code",
        "outputId": "7f77c168-26d5-4278-91cc-0143865e4544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data_train0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2937600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9xF6ziv7kN_",
        "colab_type": "code",
        "outputId": "26b5032a-4f92-441e-dc75-2ec5cc99ee13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# Un labeled\n",
        "data_train = pd.read_csv(\"/content/drive/My Drive/unlabeled_10M.tsv\",delimiter='\\t',encoding='utf-8')\n",
        "data_train.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 698000 entries, 0 to 697999\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count   Dtype \n",
            "---  ------            --------------   ----- \n",
            " 0   #1 tweet_ID       698000 non-null  int64 \n",
            " 1   #2 tweet_content  698000 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 10.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTcHFkA75tiz",
        "colab_type": "code",
        "outputId": "9b0a46bc-ca57-4de8-a809-1fda1e067896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# unlabeled = pd.read_csv(\"/content/drive/My Drive/unlabeled_10M.tsv\",delimiter='\\t',encoding='utf-8')\n",
        "dataset = pd.read_csv(\"/content/drive/My Drive/train_labeled.tsv\",delimiter='\\t',encoding='utf-8')\n",
        "\n",
        "dev_labeled = pd.read_csv(\"/content/drive/My Drive/dev_labeled.tsv\",delimiter='\\t',encoding='utf-8')\n",
        "dev_length = len(dev_labeled)\n",
        "dataset = dataset.append(dev_labeled)\n",
        "\n",
        "dataset.info()\n",
        "new_names={\"#3 country_label\":'country_label',\"#4 province_label\":\"province_label\",\"#1 tweet_ID\":\"id\",\"#2 tweet_content\":\"tweet\"}\n",
        "dataset=dataset.rename(columns=new_names)\n",
        "\n",
        "\n",
        "\n",
        "# Convert labels into integers\n",
        "label_task1 = dataset.country_label\n",
        "d = dict(zip(np.unique(label_task1), range(0,21)))\n",
        "labels_ints = dataset['country_label'].map(d, na_action='ignore')\n",
        "\n",
        "\n",
        "# Splitting\n",
        "tweets = dataset['tweet'].values\n",
        "countries = labels_ints\n",
        "\n",
        "train_tweets = tweets[:] \n",
        "train_labels = countries[:]\n",
        "\n",
        "'''val_tweets = tweets[16000:20000]\n",
        "val_labels = countries[16000:20000]\n",
        "\n",
        "test_tweets = tweets[20000:]\n",
        "test_labels = countries[20000:]\n",
        "train_tweets.shape, val_tweets.shape, test_tweets.shape\n",
        "\n",
        "# import pandas as pd \n",
        "import pandas as pd \n",
        "# Calling DataFrame constructor on list \n",
        "df = pd.DataFrame(val_labels) \n",
        "print(df)\n",
        "df.to_csv('Step1.csv',index=False)\n",
        "df = pd.read_csv('Step1.csv')\n",
        "val_labels = df['country_label']'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 25957 entries, 0 to 4956\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   #1 tweet_ID        25957 non-null  object\n",
            " 1   #2 tweet_content   25957 non-null  object\n",
            " 2   #3 country_label   25957 non-null  object\n",
            " 3   #4 province_label  25957 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 1013.9+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"val_tweets = tweets[16000:20000]\\nval_labels = countries[16000:20000]\\n\\ntest_tweets = tweets[20000:]\\ntest_labels = countries[20000:]\\ntrain_tweets.shape, val_tweets.shape, test_tweets.shape\\n\\n# import pandas as pd \\nimport pandas as pd \\n# Calling DataFrame constructor on list \\ndf = pd.DataFrame(val_labels) \\nprint(df)\\ndf.to_csv('Step1.csv',index=False)\\ndf = pd.read_csv('Step1.csv')\\nval_labels = df['country_label']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYZIOiAEGrKG",
        "colab_type": "code",
        "outputId": "c4732064-9639-40e4-c1b1-33f92a0acded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtWfj3TYheMl",
        "colab_type": "text"
      },
      "source": [
        "# Text Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR_ckYdXq63L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "\n",
        "arabic_punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             Ù‘    | # Tashdid\n",
        "                             Ù    | # Fatha\n",
        "                             Ù‹    | # Tanwin Fath\n",
        "                             Ù    | # Damma\n",
        "                             ÙŒ    | # Tanwin Damm\n",
        "                             Ù    | # Kasra\n",
        "                             Ù    | # Tanwin Kasr\n",
        "                             Ù’    | # Sukun\n",
        "                             Ù€     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "def remove_links(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
        "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
        "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
        "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
        "    return text\n",
        "\n",
        "def remove_english(text):\n",
        "  text = re.sub(\"[a-zA-Z]\",'',text)\n",
        "  return text\n",
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "def pre_process_document(text):\n",
        "  text = remove_links(text)\n",
        "  text = remove_punctuations(text)\n",
        "  text = remove_diacritics(text)\n",
        "  text = remove_repeating_char(text)\n",
        "  text = remove_english(text)\n",
        "  return text\n",
        "\n",
        "pre_process_corpus = np.vectorize(pre_process_document)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjad6KI5vhsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CLEAN FOR ELMO UNLABELED DATA\n",
        "data_train0['#2 tweet_content'] = pre_process_corpus(data_train0['#2 tweet_content'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq7g3kU48RHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CLEAN FOR ELMO UNLABELED DATA\n",
        "data_train['#2 tweet_content'] = pre_process_corpus(data_train['#2 tweet_content'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arhjudMghzJP",
        "colab_type": "code",
        "outputId": "419ba399-e0f9-413f-a3e5-a51b19ec2929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(data_train['#2 tweet_content'][697999])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT Almajlis ÙˆØ²ÙŠØ± Ø§Ù„ØªØ±Ø¨ÙŠØ© ÙˆØ§Ù„Ù†Ø§Ø¦Ø¨ Ø¯Ù…Ø­Ù…Ø¯ Ø§Ù„Ø­ÙˆÙŠÙ„Ø© ÙÙŠ Ø­ÙÙ„ ØªÙƒØ±ÙŠÙ… Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ù…Ù† Ø§Ù„Ø·Ù„Ø¨Ø© ÙˆØ§Ù„Ø·Ø§Ù„Ø¨Ø§Øª Ø§Ù„ÙƒÙˆÙŠØªÙŠÙ† Ø§Ù„Ø¯Ø§Ø±Ø³ÙŠÙ† ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ù…ØµØ±ÙŠØ© \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD22Sv3JrL6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tweets = pre_process_corpus(train_tweets)\n",
        "# val_tweets = pre_process_corpus(val_tweets)\n",
        "# test_tweets = pre_process_corpus(test_tweets)\n",
        "#train_tweets = pre_process_corpus(tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7ptdaKavsRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = data_train0[:1000000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1dCnvrNv5Ae",
        "colab_type": "code",
        "outputId": "ba4cbd80-a7d0-42e7-c4d8-3c6858ea33f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99999                                                                                                   85 Ø´Ù† ÙÙŠ Ø®ÙŠØ±Ù‡Ø§ Ø§Ù„Ø®Ù…Ø³ .\n",
              "99999    Ø¨Ù„Ø§Ø¯ÙŠ Ø¨Ù„Ø§Ø¯ÙŠ Ù„ÙƒÙŠ Ø­Ø¨ÙŠ Ùˆ ÙØ¤Ø§Ø¯ÙŠ Ø¨Ù„Ø§Ø¯ÙŠ Ø£Ø±Ø¶ Ø§Ù„ÙƒÙ†Ø§Ù†Ø© Ø£Ø±Ø¶ Ø§Ù„Ø£Ù…Ù† Ùˆ Ø§Ù„Ø£Ù…Ø§Ù† Ø£Ø±Ø¶ Ø§Ù„Ø³Ù„Ø§Ù… Ø£Ø±Ø¶ Ø§Ù„Ø£Ø¯ÙŠØ§Ù† Ø£Ø±Ø¶ Ø§Ù„Ø¬Ø¯Ø¹Ø§Ù† Ø£Ø±Ø¶ Ø§Ù„Ø«Ù‚Ø§ÙÙ‡ Ø£Ø±  .\n",
              "99999              Ø§Ù‚ØªØ±Ø¨ Ø±Ù…Ø¶Ø§Ù† Ù„ ÙŠØ¶ÙŠØ¡ Ù„Ù†Ø§ Ø´Ù‡Ø±Ø§ ÙƒØ§Ù…Ù„Ø§ Ù…ØªÙ„Ù‰Ø¡ Ø¨ Ø§Ù„Ø£Ø¬ÙˆØ§Ø¡ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†ÙŠÙ‡ Ùˆ Ø§Ù„Ù…Ø·Ù…Ø¦Ù†Ù‡ Ùˆ ÙŠØ§Ø±Ø¨ Ø£Ø¯Ø®Ù„Ù‡ Ø¹Ù„ÙŠÙ†Ø§ ÙˆØ£Ù†Øª Ø±Ø§Ø¶ Ø¹Ù†Ø§â¤ğŸŒ™ğŸ•Š .\n",
              "99999                                                     ÙƒÙ„ Ø§Ù„ÙŠ Ø¨ÙŠØ­ØµÙ„ Ø¯Ø§ Ù„Ø§Ø²Ù… ÙŠÙ‚Ù ÙƒÙØ§ÙŠØ§ Ø¯Ù… ÙƒÙØ§ÙŠØ§ Ø§Ù„ÙŠ Ù…Ø§ØªÙˆØ§ ÙŠØ§Ø±Ø¨ Ø¹Ø¯ÙŠÙ‡Ø§ Ø¹ Ø®ÙŠØ± .\n",
              "Name: #2 tweet_content, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mQFC6pJDDa2",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqgFjPikDFCD",
        "colab_type": "code",
        "outputId": "4ff74ced-7ec4-4982-c26c-84ab13f602e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "print(\"Shape of training data = \", data_train.shape)\n",
        "# data_train.sample(10)\n",
        "\n",
        "# Adding white space separated full stop to each sentence in data.\n",
        "data_train['#2 tweet_content'] = data_train['#2 tweet_content'] + \" .\"\n",
        "data_train_ = data_train['#2 tweet_content']#[1000:3000]#[300000:]\n",
        "print(data_train_)\n",
        "\n",
        "# As the training requires multiple files with one text sentence per line,\n",
        "# we will create 20K training files by writing 6 sentences per file. After running the below python snippet, we get 20K files in train directory.\n",
        "if not os.path.exists(\"/content/ELMO_TRAIN_FILES3/train/\"):\n",
        "    os.makedirs(\"/content/ELMO_TRAIN_FILES3/train/\")\n",
        " \n",
        "for i in range(0,data_train_.shape[0],10):\n",
        "    text = \"\\n\".join(data_train_[i:i+10].tolist())\n",
        "    fp = open(\"/content/ELMO_TRAIN_FILES3/train/\"+str(i)+\".txt\",\"w\")\n",
        "    fp.write(text)\n",
        "    fp.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data =  (698000, 2)\n",
            "0                                                   Ù…Ù† Ø§Ù„Ø°Ù‰ Ø³ÙŠØ­Ø¯ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„ØªÙ‰ Ø³ØªØ­ÙˆÙ„ Ù…Ù„ÙƒÙŠØªÙ‡Ø§ Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù†Ø¸ÙŠÙ Ø§Ùˆ Ø§Ù„Ø´Ø±ÙŠÙ Ø§Ùˆ Ø§Ù„Ù…Ø³Ø±ÙˆØ± Ø§Ùˆ Ø§Ù„Ù…Ø¹Ø²Ù„Ø¯ÙŠÙ† Ø§Ù„ÙˆØ·Ù†Ù‰ .\n",
            "1                                                                                                     Ù‡Ù†Ø§ Ø·Ù„Ø¹ Ø¹Ø§Ù„Ù… Ø§Ø®Ø± Ù…Ù„ÙŠØ¡ Ø¨Ø§Ø´ÙŠØ§Ø¡ Ø­Ù„ÙˆÙ‡ Ù…Ø¹ÙƒÙ… Ø¹Ù„Ù‰ Ø·ÙˆÙ„ .\n",
            "2                            Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© ØªØ¹Ø§Ù†ÙŠ Ù…Ù† Ø£Ø²Ù…Ø© Ù‡ÙˆÙŠØ© ÙˆØ£Ø²Ù…Ø© Ø¥Ù†Ø¬Ø§Ø² ÙˆÙƒÙ„ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø§Ù„ØªÙŠ ØªØ³Ù…Ø¹ÙˆÙ† Ø¨Ù‡Ø§ Ù‡ÙŠ ÙˆÙ‡Ù…ÙŠØ© ÙˆØ§Ù„Ø³Ø¨ ÙÙŠ Ø°Ù„Ùƒ ÙÙŠ Ø±Ø£ÙŠ Ù‡Ùˆ Ø§Ù„Ø¹Ø¨ÙˆØ¯ÙŠØ© Ù„Ø¢Ù„ Ø³Ø¹ÙˆØ¯ .\n",
            "3                                                                                                Ø£ØµØ¨Ø­ Ø³Ù‚ÙˆØ· Ø§Ù„Ù…Ù„ÙƒØ© Ù‚Ø§Ø¨ Ù‚ÙˆØ³ÙŠÙ† Ù‚ØµÙŠØ¯Ø© Ù†Ø¨Ø·ÙŠØ© Ø±Ø§Ø¦Ø¹Ø© ÙˆÙ‚ÙˆÙŠØ©  .\n",
            "4         ÙˆØ§Ø³ØªØ¨Ø¯ Ø§Ù„Ø§Ù…Ø±Ø§Ø¡ ÙˆØ§Ù„Ø§Ù…ÙŠØ±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø¹Ø¨ ÙˆØ¹Ø§Ø«Øª Ø§Ø³Ø±Ø© Ø§Ù„ Ø³Ø¹ÙˆØ¯ ÙØ³Ø§Ø¯Ø§ ÙØ¯Ù…Ø±Øª Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ ÙˆÙ†Ù‡Ø¨Øª Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¯ÙˆÙ„Ø© ÙˆØ§ØµØ¨Ø­Øª Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø§ÙƒØ¨Ø± Ø¯ÙˆÙ„Ø© Ù…Ø¯ÙŠÙˆÙ†Ø© Ø¹Ù„Ù‰ ÙˆØ¬Ø© Ø§Ù„Ø§ .\n",
            "                                                                              ...                                                                     \n",
            "697995                                                         Ù„Ù…Ø§ ØªÙ…Ø³Ùƒ ÙÙˆÙ† Ø­Ø¯ Ù…Ù† ØµØ­Ø§Ø¨Ùƒ ØªØ´ÙˆÙ ØµÙˆØ±Ø© Ù…Ø«Ù„Ø§ ÙˆÙ„Ø§ Ø­Ø§Ø¬Ù‡ Ù…ØªÙ‚Ù„Ø¨Ø´ ÙŠÙ…ÙŠÙ†ÙˆØ´Ù…Ø£Ù„ Ù…Ø´ Ø§Ù„Ø¨ÙˆÙ… ÙØ±Ø­ Ø§Ù…Ùƒ Ù‡Ùˆ .\n",
            "697996                                    Ù„Ø§ Ø¬Ø§ØªÙƒ Ø§Ù„Ø²Ù„Ù‡ Ù…Ù† Ø§Ù†Ø³Ø§Ù† ØºØ§Ù„ÙŠ Ù„Ù‡ Ø¯Ø§Ø®Ù„ Ø§Ø¹Ù…Ø§Ù‚Ùƒ Ù…Ø¹Ø²Ù‡ ÙˆÙ…Ù‚Ø¯Ø§Ø± Ø®Ù„Ùƒ Ø¹Ù„Ù‰ ØºÙ„Ø·Ø© Ø±ÙÙŠÙ‚Ùƒ Ø´Ù…Ø§Ù„ÙŠ ÙˆØ³Ø¹ Ù„Ù‡Ø§ ØµØ¯Ø±Ùƒ ÙˆØ¯ÙˆØ± Ù„Ù‡ Ø§Ø¹Ø°Ø§Ø± .\n",
            "697997                                                                                                 Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ù‡ ÙˆØ§Ù„Ø­Ù…Ø¯Ù„Ù‡ ÙˆÙ„Ø§ Ø¥Ù„Ù‡ Ø¥Ù„Ø§ Ø§Ù„Ù‡ ÙˆØ§Ù„Ù‡ Ø£ÙƒØ¨Ø±  .\n",
            "697998                                                                                                                                               .\n",
            "697999                           ÙˆØ²ÙŠØ± Ø§Ù„ØªØ±Ø¨ÙŠØ© ÙˆØ§Ù„Ù†Ø§Ø¦Ø¨ Ø¯Ù…Ø­Ù…Ø¯ Ø§Ù„Ø­ÙˆÙŠÙ„Ø© ÙÙŠ Ø­ÙÙ„ ØªÙƒØ±ÙŠÙ… Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ù…Ù† Ø§Ù„Ø·Ù„Ø¨Ø© ÙˆØ§Ù„Ø·Ø§Ù„Ø¨Ø§Øª Ø§Ù„ÙƒÙˆÙŠØªÙŠÙ† Ø§Ù„Ø¯Ø§Ø±Ø³ÙŠÙ† ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ù…ØµØ±ÙŠØ©  .\n",
            "Name: #2 tweet_content, Length: 698000, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY2GvkAxQI07",
        "colab_type": "code",
        "outputId": "34a2410c-47aa-4def-904f-a958c8a5ae3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_train_[299995]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'UNAVAILABLE .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWE1pt5nDMKq",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Validation Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5wcplaODYuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_dev = pd.read_csv(\"swb/swb-dev.csv\")\n",
        "# ELMO_TRAIN_FILES/valid\n",
        "data_dev = data_train#['#2 tweet_content'] = data_dev['#2 tweet_content'] + \" .\"\n",
        "\n",
        "data_dev = data_train['#2 tweet_content'][200000:250000]\n",
        "if not os.path.exists(\"ELMO_VALID_FILES1/dev/\"):\n",
        "    os.makedirs(\"ELMO_VALID_FILES1/dev/\")\n",
        " \n",
        "for i in range(0,data_dev.shape[0],10):\n",
        "    text = \"\\n\".join(data_dev[i:i+10].tolist())\n",
        "    fp = open(\"ELMO_VALID_FILES1/dev/\"+str(i)+\".txt\",\"w\")\n",
        "    fp.write(text)\n",
        "    fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc-omvt3pX59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train_ = data_train['#2 tweet_content'][:3000]#[300000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy2fm_BIDbEC",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Vocab Files\n",
        "The vocabulary file is a a text file with one token per line. It must also include the special tokens <S>, </S> and <UNK> (case sensitive) in the file. The vocabulary file should be sorted in descending order by token count in your training data. The first three lines should be the special tokens (<S>, </S> and <UNK>), then the most common token in the training data, ending with the least common token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kIF8LYdDdUX",
        "colab_type": "code",
        "outputId": "0c93ae2f-b850-4131-d6e4-1b321ffdba28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "texts = \" \".join(data_train_.tolist())\n",
        "words = texts.split(\" \")\n",
        "print(\"Number of tokens in Training data = \",len(words))\n",
        "dictionary = Counter(words)\n",
        "print(\"Size of Vocab\",len(dictionary))\n",
        "sorted_vocab = [\"<S>\",\"</S>\",\"<UNK>\"]\n",
        "sorted_vocab.extend([pair[0] for pair in dictionary.most_common()])\n",
        " \n",
        "text = \"\\n\".join(sorted_vocab)\n",
        "fp = open(\"/content/ELMO_TRAIN_FILES3/vocab.txt\",\"w\")\n",
        "fp.write(text)\n",
        "fp.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens in Training data =  10744697\n",
            "Size of Vocab 688509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bfhHPUKH3X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31G6aywcEl-p",
        "colab_type": "text"
      },
      "source": [
        "# Clone ELMO MODEL From https://github.com/allenai/bilm-tf\n",
        "And Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "megnG0AER9QS",
        "colab_type": "code",
        "outputId": "ff7d75ef-8269-41cb-8c09-2220c840ef72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM4rEHFjErT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/allenai/bilm-tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4JBNimVIOSP",
        "colab_type": "code",
        "outputId": "aa7dd8f9-930e-4810-8d60-0db672f63f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/Train_ELMO/bilm-tf/bin'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Train_ELMO/bilm-tf/bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuhmCyYKIjgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download options.json file\n",
        "%cd '/content/drive/My Drive/Train_ELMO/checkpoint/'\n",
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8Jqkm-EtpP",
        "colab_type": "code",
        "outputId": "38977a8f-ed9b-4bb5-e263-3f9d622f3e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training !!!!\n",
        "!python /content/drive/My\\ Drive/Train_ELMO/bilm-tf/bin/train_elmo.py --train_prefix='/content/ELMO_TRAIN_FILES3/train/*' --vocab_file '/content/ELMO_TRAIN_FILES3/vocab.txt' --save_dir '/content/drive/My Drive/Train_ELMO/checkpoint/'\n",
        "# Get weights file\n",
        "!python /content/drive/My\\ Drive/Train_ELMO/bilm-tf/bin/dump_weights.py --save_dir '/content/drive/My Drive/Train_ELMO/checkpoint/' --outfile '/content/drive/My Drive/Train_ELMO/ELMO_weights_1Million_SentPerFile_Arabic.hdf5'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:21: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:21: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "Found 69800 shards at /content/ELMO_TRAIN_FILES3/train/*\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/556630.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Found 69800 shards at /content/ELMO_TRAIN_FILES3/train/*\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/436230.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:684: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:690: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:702: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:153: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:224: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:211: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "USING SKIP CONNECTIONS\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:372: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:390: The name tf.nn.rnn_cell.DropoutWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.DropoutWrapper instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:386: The name tf.nn.rnn_cell.ResidualWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.ResidualWrapper instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:396: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:410: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:424: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:425: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:26: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "[['global_step:0', TensorShape([])],\n",
            " ['lm/CNN/W_cnn_0:0',\n",
            "  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],\n",
            " ['lm/CNN/W_cnn_1:0',\n",
            "  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],\n",
            " ['lm/CNN/W_cnn_2:0',\n",
            "  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],\n",
            " ['lm/CNN/W_cnn_3:0',\n",
            "  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],\n",
            " ['lm/CNN/W_cnn_4:0',\n",
            "  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],\n",
            " ['lm/CNN/W_cnn_5:0',\n",
            "  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],\n",
            " ['lm/CNN/W_cnn_6:0',\n",
            "  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],\n",
            " ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],\n",
            " ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],\n",
            " ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],\n",
            " ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],\n",
            " ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],\n",
            " ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],\n",
            " ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],\n",
            " ['lm/CNN_high_0/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],\n",
            " ['lm/CNN_high_0/W_transform:0',\n",
            "  TensorShape([Dimension(2048), Dimension(2048)])],\n",
            " ['lm/CNN_high_0/b_carry:0', TensorShape([Dimension(2048)])],\n",
            " ['lm/CNN_high_0/b_transform:0', TensorShape([Dimension(2048)])],\n",
            " ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(128)])],\n",
            " ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(128)])],\n",
            " ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',\n",
            "  TensorShape([Dimension(4096)])],\n",
            " ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',\n",
            "  TensorShape([Dimension(256), Dimension(4096)])],\n",
            " ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0',\n",
            "  TensorShape([Dimension(1024), Dimension(128)])],\n",
            " ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',\n",
            "  TensorShape([Dimension(4096)])],\n",
            " ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',\n",
            "  TensorShape([Dimension(256), Dimension(4096)])],\n",
            " ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0',\n",
            "  TensorShape([Dimension(1024), Dimension(128)])],\n",
            " ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',\n",
            "  TensorShape([Dimension(4096)])],\n",
            " ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',\n",
            "  TensorShape([Dimension(256), Dimension(4096)])],\n",
            " ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0',\n",
            "  TensorShape([Dimension(1024), Dimension(128)])],\n",
            " ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',\n",
            "  TensorShape([Dimension(4096)])],\n",
            " ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',\n",
            "  TensorShape([Dimension(256), Dimension(4096)])],\n",
            " ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0',\n",
            "  TensorShape([Dimension(1024), Dimension(128)])],\n",
            " ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],\n",
            " ['lm/softmax/W:0', TensorShape([Dimension(688512), Dimension(128)])],\n",
            " ['lm/softmax/b:0', TensorShape([Dimension(688512)])],\n",
            " ['train_perplexity:0', TensorShape([])]]\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:639: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:904: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:910: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:732: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:735: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:595: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:755: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:756: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:765: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:765: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2020-04-18 16:52:24.149422: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-04-18 16:52:24.149810: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16fa680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-04-18 16:52:24.149851: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-04-18 16:52:24.151977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-04-18 16:52:24.223812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-18 16:52:24.224799: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16fa840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-04-18 16:52:24.224834: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-04-18 16:52:24.225039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-18 16:52:24.225811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-04-18 16:52:24.226129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-04-18 16:52:24.227873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-04-18 16:52:24.229480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-04-18 16:52:24.229823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-04-18 16:52:24.231736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-04-18 16:52:24.232782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-04-18 16:52:24.239508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-04-18 16:52:24.239657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-18 16:52:24.240476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-18 16:52:24.241163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-04-18 16:52:24.241223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-04-18 16:52:24.242583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-04-18 16:52:24.242614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-04-18 16:52:24.242623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-04-18 16:52:24.242762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-18 16:52:24.243653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-18 16:52:24.244337: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-04-18 16:52:24.244380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/training.py:774: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'list' object has no attribute 'name'\n",
            "Training for 10 epochs and 41970 batches\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/204650.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/356210.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/378040.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/135330.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/318350.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/673460.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/302910.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/413090.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/100190.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/617340.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/322040.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/556880.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/80040.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/24170.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/384200.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/497840.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/335570.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/541280.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/164920.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/623000.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/334710.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/29280.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/108100.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/386000.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/140800.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/462360.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/226360.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/399120.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/92240.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/59880.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/230920.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/333470.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/391970.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/5560.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/119870.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/102090.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/28950.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/543540.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/230820.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/437960.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/383240.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/637630.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/541700.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/248300.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/220220.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/55230.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "Loading data from: /content/ELMO_TRAIN_FILES3/train/259500.txt\n",
            "Loaded 10 sentences.\n",
            "Finished loading\n",
            "2020-04-18 16:52:34.765695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-04-18 16:52:36.257609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzYPzS7os9NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restart - Training !!!!\n",
        "!python /content/drive/My\\ Drive/Train_ELMO/bilm-tf/bin/restart.py --train_prefix='/content/ELMO_TRAIN_FILES3/train/*' --vocab_file '/content/ELMO_TRAIN_FILES1/vocab.txt' --save_dir '/content/drive/My Drive/Train_ELMO/checkpoint/'\n",
        "# Get weights file\n",
        "#!python /content/drive/My\\ Drive/Train_ELMO/bilm-tf/bin/dump_weights.py --save_dir '/content/drive/My Drive/Train_ELMO/checkpoint/' --outfile '/content/drive/My Drive/Train_ELMO/ELMO_weights700k_10SentPerFile_Arabic.hdf5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ihr8CVQg0Ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /content/drive/My\\ Drive/Train_ELMO/bilm-tf/bin/dump_weights.py --save_dir '/content/drive/My Drive/Train_ELMO/checkpoint/' --outfile '/content/drive/My Drive/Train_ELMO/ELMO_weights_test_weigths.hdf5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loFpHo1FS7ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating!!! ( SKIP )\n",
        "!python /content/drive/My\\ Drive/Train_ELMO/bilm-tf/bin/run_test.py --test_prefix='/content/ELMO_VALID_FILES2/dev/*' --vocab_file '/content/ELMO_TRAIN_FILES2/vocab.txt' --save_dir='/content/drive/My Drive/Train_ELMO/checkpoint/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjhE0b4dVWU2",
        "colab_type": "text"
      },
      "source": [
        "# Convert Tf Checkpoint to hdf5\n",
        "\n",
        "This step is important to convert the checkpoint model to hdf5 format which can be easily used for inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AIL18xGVQcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get weights file\n",
        "!python /content/drive/My\\ Drive/Train_ELMO/bilm-tf/bin/dump_weights.py --save_dir '/content/drive/My Drive/Train_ELMO/checkpoint/' --outfile '/content/drive/My Drive/Train_ELMO/ELMO_weights200k_Arabic.hdf5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc7VAuz6Y4EU",
        "colab_type": "text"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Now,\n",
        "\n",
        "    Keep the dumped weights file in newly created model folder.\n",
        "    Create an options.json file for the newly trained model in same folder.\n",
        "    It is important to always set n_characters to 262 after training.\n",
        "    Keep vocab.txt in model directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4YJ0HQHY53k",
        "colab_type": "code",
        "outputId": "0ae172d4-f45b-44bb-ac5e-8da216a1447f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.spatial.distance as ds\n",
        "from data import Batcher \n",
        "from model import BidirectionalLanguageModel\n",
        "from elmo import weight_layers\n",
        " \n",
        "# Location of pretrained LM.  Here we use the test fixtures.\n",
        "datadir = os.path.join('/content/drive/My Drive/', 'Train_ELMO')\n",
        "vocab_file = os.path.join(datadir, 'vocab.txt')\n",
        "options_file = os.path.join(datadir+'/checkpoint/', 'options.json')\n",
        "weight_file = os.path.join(datadir, 'ELMO_weights_test_weigths.hdf5')\n",
        "# Create a Batcher to map text to character ids.\n",
        "batcher = Batcher(vocab_file, 50)\n",
        " \n",
        "# Input placeholders to the biLM.\n",
        "context_character_ids = tf.placeholder('int32', shape=(None, None, 50))\n",
        " \n",
        "# Build the biLM graph.\n",
        "bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
        " \n",
        "# Get ops to compute the LM embeddings.\n",
        "context_embeddings_op = bilm(context_character_ids)\n",
        " \n",
        "# Get an op to compute ELMo (weighted average of the internal biLM layers)\n",
        "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)\n",
        "\n",
        "tokenized_context = [sentence.split() for sentence in train_tweets]\n",
        "'''\n",
        "# Now we can compute embeddings.\n",
        "raw_context = ['Technology has advanced so much in new scientific world',\n",
        "                'My child participated in fancy dress competition',\n",
        "                'Fashion industry has seen tremendous growth in new designs']\n",
        "'''\n",
        "\n",
        "# print(tokenized_context)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/model.py:333: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/model.py:378: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "USING SKIP CONNECTIONS\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/model.py:524: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/model.py:568: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/model.py:569: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/model.py:593: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/model.py:538: The name tf.nn.rnn_cell.ResidualWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.ResidualWrapper instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/elmo.py:94: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Train_ELMO/bilm-tf/bin/elmo.py:95: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Now we can compute embeddings.\\nraw_context = ['Technology has advanced so much in new scientific world',\\n                'My child participated in fancy dress competition',\\n                'Fashion industry has seen tremendous growth in new designs']\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkzVq8nqvJI7",
        "colab_type": "code",
        "outputId": "7793276d-e412-4608-fd98-4fb764985b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "raw_context2 = ['ØµØ¨Ø§Ø­ Ø§Ù„ÙÙ„ ÙŠØ²Ù…ÙŠÙ„ÙŠ',\n",
        "                'Ù…Ø³Ø§ Ù…Ø³Ø§ ÙŠØ§ Ø§Ø¨Ø±Ø§Ù‡ÙŠÙŠÙ… Ø®Ù„ØµØ§Ù†Ø© Ø¨Ø´ÙŠØ§ÙƒØ©',\n",
        "                'Ø§ÙŠÙ‡ ØªØ§Ù‡ Ø§ÙŠÙ‡ ØªØ§Ù‡ Ø§Ù„Ø­Ù‚Ùˆ']\n",
        "\n",
        "tokenized_context2 = [sentence.split() for sentence in raw_context2]\n",
        "print(tokenized_context2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['ØµØ¨Ø§Ø­', 'Ø§Ù„ÙÙ„', 'ÙŠØ²Ù…ÙŠÙ„ÙŠ'], ['Ù…Ø³Ø§', 'Ù…Ø³Ø§', 'ÙŠØ§', 'Ø§Ø¨Ø±Ø§Ù‡ÙŠÙŠÙ…', 'Ø®Ù„ØµØ§Ù†Ø©', 'Ø¨Ø´ÙŠØ§ÙƒØ©'], ['Ø§ÙŠÙ‡', 'ØªØ§Ù‡', 'Ø§ÙŠÙ‡', 'ØªØ§Ù‡', 'Ø§Ù„Ø­Ù‚Ùˆ', 'ÙŠØ§', 'Ø¨Ù†Ø§Øª', 'Ø§Ù‡', 'ÙŠØ¨Ù†', 'Ø§Ù„']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Vbq90N2uGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_context.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtM_Lv8zX1wm",
        "colab_type": "code",
        "outputId": "2c7c8ce3-cbc9-49fa-d7e1-eb17da05cd03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "elmo_context_input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'regularization_op': [<tf.Tensor 'input_ELMo_W/Regularizer/mul:0' shape=() dtype=float32>],\n",
              " 'weighted_op': <tf.Tensor 'mul_3:0' shape=(?, ?, 256) dtype=float32>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS3okW9aYdU-",
        "colab_type": "code",
        "outputId": "0e78e71f-6712-48e2-c1c7-6f17d0f1a4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "context_ids.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16000, 64, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwS-HcF0YyTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_embds(sess,context_ids__):\n",
        "   return sess.run(\n",
        "        elmo_context_input['weighted_op'],\n",
        "        feed_dict={context_character_ids: context_ids__} )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waef8h0jZG-S",
        "colab_type": "code",
        "outputId": "50f40fcd-dbf9-450b-f8f8-361c94935c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Train DATA\n",
        "with tf.Session() as sess:\n",
        "    # It is necessary to initialize variables once before running inference.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        " \n",
        "    # Create batches of data.\n",
        "    context_ids = batcher.batch_sentences(tokenized_context)\n",
        "    print(\"Shape of context ids = \", context_ids.shape)\n",
        "    context_ids_1 = [context_ids[i:i+100] for i in range(0,context_ids.shape[0],100)]\n",
        "\n",
        "    # context_ids_1 = context_ids[:100]\n",
        "    # Compute ELMo representations (here for the input only, for simplicity).\n",
        "    elmo_context_input_ = [compute_embds(sess, x) for x in context_ids_1]\n",
        "    \n",
        "    elmo_context_input_ = np.concatenate(elmo_context_input_, axis = 0)\n",
        "print(\"Shape of generated embeddings = \",elmo_context_input_.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of context ids =  (25957, 64, 50)\n",
            "Shape of generated embeddings =  (25957, 62, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPx5sk_k7Iu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_context_input_ = np.reshape(elmo_context_input_,(25957,62*256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLoQk973bwjU",
        "colab_type": "code",
        "outputId": "9e32aea9-f838-445d-e3f9-118c7e969f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(elmo_context_input_))\n",
        "print(len(train_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25957\n",
            "25957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7wOwoWwIf2d",
        "colab_type": "text"
      },
      "source": [
        "# Skip these"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_rtWvtIaz4Q",
        "colab_type": "code",
        "outputId": "e4170678-0313-46d8-c665-77efc26331ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# ( SKIP )\n",
        "#Validation tweets\n",
        "tokenized_context_val = [sentence.split() for sentence in val_tweets]\n",
        "#print(tokenized_context_test)\n",
        "with tf.Session() as sess:\n",
        "    # It is necessary to initialize variables once before running inference.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        " \n",
        "    # Create batches of data.\n",
        "    context_ids_val = batcher.batch_sentences(tokenized_context_val)\n",
        "    print(\"Shape of context ids = \", context_ids_val.shape)\n",
        "    context_ids_2 = [context_ids_val[i:i+100] for i in range(0,context_ids_val.shape[0],100)]\n",
        "\n",
        "    # context_ids_1 = context_ids[:100]\n",
        "    # Compute ELMo representations (here for the input only, for simplicity).\n",
        "    elmo_context_input_val = [compute_embds(sess, x) for x in context_ids_2]\n",
        "    elmo_context_input_val = np.concatenate(elmo_context_input_val, axis = 0)\n",
        "print(\"Shape of generated embeddings = \",elmo_context_input_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of context ids =  (4000, 62, 50)\n",
            "Shape of generated embeddings =  (4000, 60, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mggz9REeadXU",
        "colab_type": "code",
        "outputId": "ceb620f0-cc3f-4bd6-e38a-cd43cd9351b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# ( SKIP )\n",
        "tokenized_context_test = [sentence.split() for sentence in test_tweets]\n",
        "#print(tokenized_context_test)\n",
        "with tf.Session() as sess:\n",
        "    # It is necessary to initialize variables once before running inference.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        " \n",
        "    # Create batches of data.\n",
        "    context_ids_test = batcher.batch_sentences(tokenized_context_test)\n",
        "    print(\"Shape of context ids = \", context_ids_test.shape)\n",
        "    context_ids_3 = [context_ids_test[i:i+100] for i in range(0,context_ids_test.shape[0],100)]\n",
        "\n",
        "    # context_ids_1 = context_ids[:100]\n",
        "    # Compute ELMo representations (here for the input only, for simplicity).\n",
        "    elmo_context_input_test = [compute_embds(sess, x) for x in context_ids_3]\n",
        "    elmo_context_input_test = np.concatenate(elmo_context_input_test, axis = 0)\n",
        "print(\"Shape of generated embeddings = \",elmo_context_input_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of context ids =  (1000, 60, 50)\n",
            "Shape of generated embeddings =  (1000, 58, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqTEpZCHIr9l",
        "colab_type": "code",
        "outputId": "19ffe31e-2c32-4f59-b37d-951b6faafff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(elmo_context_input_.shape)\n",
        "print(elmo_context_input_val.shape)\n",
        "print(elmo_context_input_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16000, 62, 256)\n",
            "(4000, 60, 256)\n",
            "(1000, 58, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYFvYP4aE1Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshaping\n",
        "elmo_context_input_ = np.reshape(elmo_context_input_,(16000,62*256))\n",
        "elmo_context_input_val = np.reshape(elmo_context_input_val,(4000, 60*256))\n",
        "elmo_context_input_test = np.reshape(elmo_context_input_test, (1000, 58*256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf0oRxOUZT11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.shape(elmo_context_input_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRMb8EMs5dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_context_input_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tVMldRhZMQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computing euclidean distance between words embedding\n",
        "euc_dist_bet_tech_computer = np.linalg.norm(elmo_context_input_[1,5,:]-elmo_context_input_[0,0,:])\n",
        "euc_dist_bet_computer_fashion = np.linalg.norm(elmo_context_input_[1,5,:]-elmo_context_input_[2,0,:])\n",
        "# Computing cosine distance between words embedding\n",
        "cos_dist_bet_tech_computer = ds.cosine(elmo_context_input_[1,5,:],elmo_context_input_[0,0,:])\n",
        "cos_dist_bet_computer_fashion = ds.cosine(elmo_context_input_[1,5,:],elmo_context_input_[2,0,:])\n",
        " \n",
        "print(\"Euclidean Distance Comparison - \")\n",
        "print(\"\\nDress-Technology = \",np.round(euc_dist_bet_tech_computer,2),\"\\nDress-Fashion = \",\n",
        "      np.round(euc_dist_bet_computer_fashion,2))\n",
        "print(\"\\n\\nCosine Distance Comparison - \")\n",
        "print(\"\\nDress-Technology = \",np.round(cos_dist_bet_tech_computer,2),\"\\nDress-Fashion = \",\n",
        "      np.round(cos_dist_bet_computer_fashion,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjY_sxqSZTmE",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXa83_WQZWHF",
        "colab_type": "code",
        "outputId": "d744d7c5-8b40-437b-addd-9f956be7436a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split data back to train and valid\n",
        "X_train, X_test, y_train, y_test = train_test_split(elmo_context_input_, train_labels, test_size=0.2)\n",
        "'''X_train = elmo_context_input_[:21000]\n",
        "y_train = train_labels[:21000]\n",
        "X_test = elmo_context_input_[21000:]\n",
        "y_test = train_labels[21000:]'''\n",
        "print(\"Splitted.\")\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(X_train, y_train)\n",
        "print(\"trained\")\n",
        "preds = lreg.predict(X_test)\n",
        "#preds_valid = lreg.predict(xvalid)\n",
        "print(\"Acc = \", accuracy_score(preds, y_test))\n",
        "print(\"F1-Macro\",f1_score(y_test, preds,average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitted.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trained\n",
            "Acc =  0.21302003081664098\n",
            "F1-Macro 0.05349712811582373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKNjLqQ8TUTg",
        "colab_type": "code",
        "outputId": "650c6be0-0295-4d88-d298-e9bbf23ce465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f1_score(y_test, preds,average='micro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2235714285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Sw2hhNLZso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import Gaussian Naive Bayes model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "NB = MultinomialNB()\n",
        "\n",
        "# Train the model using the training sets\n",
        "NB.fit(X_train, y_train)\n",
        "\n",
        "#Predict Output\n",
        "preds = NB.predict(X_test)\n",
        "#preds_valid = lreg.predict(xvalid)\n",
        "print(\"Acc = \", accuracy_score(preds, y_test))\n",
        "print(\"F1-Macro\",f1_score(y_test, preds,average='macro'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apkuX2AW8fEx",
        "colab_type": "text"
      },
      "source": [
        "First trained on 200k unlabeled F1-scores:\n",
        "0.20333333333333334\n",
        "0.10323744491762131\n",
        "\n",
        "2nd trained on 300k unlabeled F1-scores:\n",
        "0.22357142857142856\n",
        "0.1309353180237549"
      ]
    }
  ]
}